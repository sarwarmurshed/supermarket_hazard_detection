{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, time\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of GPUs to use\n",
    "num_gpus = 1\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "b_size = 32\n",
    "# Get the model ResNet50_v2, with 10 output classes\n",
    "# net = get_model('ResNet50_v2', classes=1000)\n",
    "# net.initialize(mx.init.MSRAPrelu(), ctx = ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter_param = 0.4\n",
    "lighting_param = 0.1\n",
    "mean_rgb = [123.68, 116.779, 103.939]\n",
    "std_rgb = [58.393, 57.12, 57.375]\n",
    "\n",
    "train_data = mx.io.ImageRecordIter(\n",
    "    path_imgrec         = './data/rec/train.rec',\n",
    "    path_imgidx         = './data/rec/train.idx',\n",
    "    preprocess_threads  = 32,\n",
    "    shuffle             = True,\n",
    "    batch_size          = b_size,\n",
    "\n",
    "    data_shape          = (3, 224, 224),\n",
    "    mean_r              = mean_rgb[0],\n",
    "    mean_g              = mean_rgb[1],\n",
    "    mean_b              = mean_rgb[2],\n",
    "    std_r               = std_rgb[0],\n",
    "    std_g               = std_rgb[1],\n",
    "    std_b               = std_rgb[2],\n",
    "    rand_mirror         = True,\n",
    "    random_resized_crop = True,\n",
    "    max_aspect_ratio    = 4. / 3.,\n",
    "    min_aspect_ratio    = 3. / 4.,\n",
    "    max_random_area     = 1,\n",
    "    min_random_area     = 0.08,\n",
    "    brightness          = jitter_param,\n",
    "    saturation          = jitter_param,\n",
    "    contrast            = jitter_param,\n",
    "    pca_noise           = lighting_param,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = mx.io.ImageRecordIter(\n",
    "    path_imgrec         = './data/rec/val.rec',\n",
    "    path_imgidx         = './data/rec/val.idx',\n",
    "    preprocess_threads  = 32,\n",
    "    shuffle             = False,\n",
    "    batch_size          = b_size,\n",
    "\n",
    "    resize              = 256,\n",
    "    data_shape          = (3, 224, 224),\n",
    "    mean_r              = mean_rgb[0],\n",
    "    mean_g              = mean_rgb[1],\n",
    "    mean_b              = mean_rgb[2],\n",
    "    std_r               = std_rgb[0],\n",
    "    std_g               = std_rgb[1],\n",
    "    std_b               = std_rgb[2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements.  See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership.  The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied.  See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License.\n",
    "\n",
    "\n",
    "__all__ = ['EdgeLiteNet', 'edgeLiteNet']\n",
    "\n",
    "from mxnet.context import cpu\n",
    "from mxnet.gluon.block import HybridBlock\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.nn import BatchNorm\n",
    "from mxnet.gluon.contrib.nn import HybridConcurrent\n",
    "\n",
    "def _make_basic_conv(in_channels, channels, norm_layer=BatchNorm, norm_kwargs=None, **kwargs):\n",
    "    out = nn.HybridSequential(prefix='')\n",
    "    out.add(nn.Conv2D(in_channels=in_channels, channels=channels, use_bias=False, **kwargs))\n",
    "    out.add(norm_layer(in_channels=channels, epsilon=0.001,\n",
    "                       **({} if norm_kwargs is None else norm_kwargs)))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    return out\n",
    "\n",
    "def _make_branch(use_pool, norm_layer, norm_kwargs, *conv_settings):\n",
    "    out = nn.HybridSequential(prefix='')\n",
    "    if use_pool == 'avg':\n",
    "        out.add(nn.AvgPool2D(pool_size=3, strides=1, padding=1))\n",
    "    elif use_pool == 'max':\n",
    "        out.add(nn.MaxPool2D(pool_size=3, strides=1, padding=1))\n",
    "    setting_names = ['in_channels', 'channels', 'kernel_size', 'strides', 'padding']\n",
    "    for setting in conv_settings:\n",
    "        kwargs = {}\n",
    "        for i, value in enumerate(setting):\n",
    "            if value is not None:\n",
    "                if setting_names[i] == 'in_channels':\n",
    "                    in_channels = value\n",
    "                elif setting_names[i] == 'channels':\n",
    "                    channels = value\n",
    "                else:\n",
    "                    kwargs[setting_names[i]] = value\n",
    "        out.add(_make_basic_conv(in_channels, channels, norm_layer, norm_kwargs, **kwargs))\n",
    "    return out\n",
    "\n",
    "def _make_Mixed_4a(in_channels, pool_features, prefix, norm_layer, norm_kwargs):\n",
    "    out = HybridConcurrent(axis=1, prefix=prefix)\n",
    "    with out.name_scope():\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 192, 1, None, None)))\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 96, 1, None, None),\n",
    "                             (96, 208, 3, None, 1)))\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 16, 1, None, None),\n",
    "                             (16, 48, 3, None, 1)))\n",
    "        out.add(_make_branch('max', norm_layer, norm_kwargs,\n",
    "                             (in_channels, pool_features, 1, None, None)))\n",
    "    return out\n",
    "\n",
    "def _make_Mixed_4b(in_channels, pool_features, prefix, norm_layer, norm_kwargs):\n",
    "    out = HybridConcurrent(axis=1, prefix=prefix)\n",
    "    with out.name_scope():\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 160, 1, None, None)))\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 112, 1, None, None),\n",
    "                             (112, 224, 3, None, 1)))\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 24, 1, None, None),\n",
    "                             (24, 64, 3, None, 1)))\n",
    "        out.add(_make_branch('max', norm_layer, norm_kwargs,\n",
    "                             (in_channels, pool_features, 1, None, None)))\n",
    "    return out\n",
    "\n",
    "def _make_Mixed_4c(in_channels, pool_features, prefix, norm_layer, norm_kwargs):\n",
    "    out = HybridConcurrent(axis=1, prefix=prefix)\n",
    "    with out.name_scope():\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 128, 1, None, None)))\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 128, 1, None, None),\n",
    "                             (128, 256, 3, None, 1)))\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 24, 1, None, None),\n",
    "                             (24, 64, 3, None, 1)))\n",
    "        out.add(_make_branch('max', norm_layer, norm_kwargs,\n",
    "                             (in_channels, pool_features, 1, None, None)))\n",
    "    return out\n",
    "\n",
    "def _make_Mixed_4d(in_channels, pool_features, prefix, norm_layer, norm_kwargs):\n",
    "    out = HybridConcurrent(axis=1, prefix=prefix)\n",
    "    with out.name_scope():\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 112, 1, None, None)))\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 144, 1, None, None),\n",
    "                             (144, 288, 3, None, 1)))\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 32, 1, None, None),\n",
    "                             (32, 64, 3, None, 1)))\n",
    "        out.add(_make_branch('max', norm_layer, norm_kwargs,\n",
    "                             (in_channels, pool_features, 1, None, None)))\n",
    "    return out\n",
    "\n",
    "def _make_Mixed_4e(in_channels, pool_features, prefix, norm_layer, norm_kwargs):\n",
    "    out = HybridConcurrent(axis=1, prefix=prefix)\n",
    "    with out.name_scope():\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 256, 1, None, None)))\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 160, 1, None, None),\n",
    "                             (160, 320, 3, None, 1)))\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 32, 1, None, None),\n",
    "                             (32, 128, 3, None, 1)))\n",
    "        out.add(_make_branch('max', norm_layer, norm_kwargs,\n",
    "                             (in_channels, pool_features, 1, None, None)))\n",
    "    return out\n",
    "\n",
    "def _make_Mixed_5a(in_channels, pool_features, prefix, norm_layer, norm_kwargs):\n",
    "    out = HybridConcurrent(axis=1, prefix=prefix)\n",
    "    with out.name_scope():\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 256, 1, None, None)))\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 160, 1, None, None),\n",
    "                             (160, 320, 3, None, 1)))\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 32, 1, None, None),\n",
    "                             (32, 128, 3, None, 1)))\n",
    "        out.add(_make_branch('max', norm_layer, norm_kwargs,\n",
    "                             (in_channels, pool_features, 1, None, None)))\n",
    "    return out\n",
    "\n",
    "def _make_Mixed_5b(in_channels, pool_features, prefix, norm_layer, norm_kwargs):\n",
    "    out = HybridConcurrent(axis=1, prefix=prefix)\n",
    "    with out.name_scope():\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 384, 1, None, None)))\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 192, 1, None, None),\n",
    "                             (192, 384, 3, None, 1)))\n",
    "        out.add(_make_branch(None, norm_layer, norm_kwargs,\n",
    "                             (in_channels, 48, 1, None, None),\n",
    "                             (48, 128, 3, None, 1)))\n",
    "        out.add(_make_branch('max', norm_layer, norm_kwargs,\n",
    "                             (in_channels, pool_features, 1, None, None)))\n",
    "    return out\n",
    "\n",
    "def _make_aux(in_channels, classes, norm_layer, norm_kwargs):\n",
    "    out = nn.HybridSequential(prefix='')\n",
    "    out.add(nn.AvgPool2D(pool_size=5, strides=3))\n",
    "    out.add(_make_basic_conv(in_channels=in_channels, channels=128, kernel_size=1,\n",
    "                             norm_layer=norm_layer, norm_kwargs=norm_kwargs))\n",
    "\n",
    "    out.add(nn.Flatten())\n",
    "    out.add(nn.Dense(units=1024, in_units=2048))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Dropout(0.7))\n",
    "    out.add(nn.Dense(units=classes, in_units=1024))\n",
    "    return out\n",
    "\n",
    "class EdgeLiteNet(HybridBlock):\n",
    "\n",
    "    def __init__(self, classes=1000, norm_layer=BatchNorm, dropout_ratio=0.4, aux_logits=False,\n",
    "                 norm_kwargs=None, partial_bn=False, pretrained_base=True, ctx=None, **kwargs):\n",
    "        super(EdgeLiteNet, self).__init__(**kwargs)\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.aux_logits = aux_logits\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.conv1 = _make_basic_conv(in_channels=3, channels=64, kernel_size=7,\n",
    "                                          strides=2, padding=3,\n",
    "                                          norm_layer=norm_layer, norm_kwargs=norm_kwargs)\n",
    "            self.maxpool1 = nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True)\n",
    "\n",
    "            if partial_bn:\n",
    "                if norm_kwargs is not None:\n",
    "                    norm_kwargs['use_global_stats'] = True\n",
    "                else:\n",
    "                    norm_kwargs = {}\n",
    "                    norm_kwargs['use_global_stats'] = True\n",
    "\n",
    "            self.conv2 = _make_basic_conv(in_channels=64, channels=64, kernel_size=1,\n",
    "                                          norm_layer=norm_layer, norm_kwargs=norm_kwargs)\n",
    "            self.conv3 = _make_basic_conv(in_channels=64, channels=192,\n",
    "                                          kernel_size=3, padding=1,\n",
    "                                          norm_layer=norm_layer, norm_kwargs=norm_kwargs)\n",
    "            self.maxpool2 = nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True)\n",
    "\n",
    "            self.edgelite4a = _make_Mixed_4a(192, 64, 'Mixed_4a_', norm_layer, norm_kwargs)\n",
    "            self.edgelite4b = _make_Mixed_4b(512, 64, 'Mixed_4b_', norm_layer, norm_kwargs)\n",
    "            self.edgelite4c = _make_Mixed_4c(512, 64, 'Mixed_4c_', norm_layer, norm_kwargs)\n",
    "            self.edgelite4d = _make_Mixed_4d(512, 64, 'Mixed_4d_', norm_layer, norm_kwargs)\n",
    "            self.edgelite4e = _make_Mixed_4e(528, 128, 'Mixed_4e_', norm_layer, norm_kwargs)\n",
    "            self.maxpool4 = nn.MaxPool2D(pool_size=2, strides=2)\n",
    "\n",
    "            self.edgelite5a = _make_Mixed_5a(832, 128, 'Mixed_5a_', norm_layer, norm_kwargs)\n",
    "            self.edgelite5b = _make_Mixed_5b(832, 128, 'Mixed_5b_', norm_layer, norm_kwargs)\n",
    "\n",
    "            if self.aux_logits:\n",
    "                self.aux1 = _make_aux(512, classes, norm_layer, norm_kwargs)\n",
    "                self.aux2 = _make_aux(528, classes, norm_layer, norm_kwargs)\n",
    "\n",
    "            self.head = nn.HybridSequential(prefix='')\n",
    "            self.avgpool = nn.AvgPool2D(pool_size=7)\n",
    "            self.dropout = nn.Dropout(self.dropout_ratio)\n",
    "            #self.dropout = nn.Dropout(self.dropout_ratio)\n",
    "            self.output = nn.Dense(units=classes, in_units=4096)\n",
    "            #self.output = nn.Dense(units=classes, in_units=1024)\n",
    "            self.head.add(self.avgpool)\n",
    "            self.head.add(self.dropout)\n",
    "            self.head.add(self.output)\n",
    "    def hybrid_forward(self, F, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.edgelite4a(x)\n",
    "        if self.aux_logits:\n",
    "            aux1 = self.aux1(x)\n",
    "        x = self.edgelite4b(x)\n",
    "        x = self.edgelite4c(x)\n",
    "        x = self.edgelite4d(x)\n",
    "        if self.aux_logits:\n",
    "            aux2 = self.aux2(x)\n",
    "        x = self.edgelite4e(x)\n",
    "        x = self.maxpool4(x)\n",
    "\n",
    "        x = self.edgelite5a(x)\n",
    "        x = self.edgelite5b(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        if self.aux_logits:\n",
    "            return (x, aux2, aux1)\n",
    "        return x\n",
    "\n",
    "\n",
    "def edgeLiteNet(classes=1000, pretrained=False, pretrained_base=True, ctx=cpu(),\n",
    "              dropout_ratio=0.4, aux_logits=False,\n",
    "              root='~/.mxnet/models', partial_bn=False, **kwargs):\n",
    "    net = EdgeLiteNet(classes=classes, partial_bn=partial_bn, pretrained_base=pretrained_base,\n",
    "                    dropout_ratio=dropout_ratio, aux_logits=aux_logits, ctx=ctx, **kwargs)\n",
    "    if pretrained:\n",
    "        from .model_store import get_model_file\n",
    "        net.load_parameters(get_model_file('edgeLiteNet',\n",
    "                                           tag=pretrained, root=root), ctx=ctx, cast_dtype=True)\n",
    "        from ..data import ImageNet1kAttr\n",
    "        attrib = ImageNet1kAttr()\n",
    "        net.synset = attrib.synset\n",
    "        net.classes = attrib.classes\n",
    "        net.classes_long = attrib.classes_long\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = edgeLiteNet(classes=1000)\n",
    "# Initialize the parameters with Xavier initializer\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "# Epochs where learning rate decays\n",
    "lr_decay_epoch = [30, 60, 90, np.inf]\n",
    "\n",
    "# Nesterov accelerated gradient descent\n",
    "optimizer = 'nag'\n",
    "# Set parameters\n",
    "optimizer_params = {'learning_rate': 0.1, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_top1 = mx.metric.Accuracy()\n",
    "acc_top5 = mx.metric.TopKAccuracy(5)\n",
    "train_history = TrainingHistory(['training-top1-err', 'training-top5-err',\n",
    "                                 'validation-top1-err', 'validation-top5-err'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ctx, val_data):\n",
    "    acc_top1_val = mx.metric.Accuracy()\n",
    "    acc_top5_val = mx.metric.TopKAccuracy(5)\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X) for X in data]\n",
    "        acc_top1_val.update(label, outputs)\n",
    "        acc_top5_val.update(label, outputs)\n",
    "\n",
    "    _, top1 = acc_top1_val.get()\n",
    "    _, top5 = acc_top5_val.get()\n",
    "    return (1 - top1, 1 - top5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "lr_decay_count = 0\n",
    "log_interval = 50\n",
    "lr_decay_period = 0\n",
    "batch_size = b_size\n",
    "\n",
    "net.hybridize()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    btic = time.time()\n",
    "    acc_top1.reset()\n",
    "    acc_top5.reset()\n",
    "\n",
    "    if lr_decay_period == 0 and epoch == lr_decay_epoch[lr_decay_count]:\n",
    "        trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
    "        lr_decay_count += 1\n",
    "\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "        with ag.record():\n",
    "#             for X in data:\n",
    "#                 print (X)\n",
    "            outputs = [net(X) for X in data]\n",
    "            loss = [L(yhat, y) for yhat, y in zip(outputs, label)]\n",
    "        ag.backward(loss)\n",
    "        trainer.step(batch_size, ignore_stale_grad=True)\n",
    "        acc_top1.update(label, outputs)\n",
    "        acc_top5.update(label, outputs)\n",
    "        if log_interval and not (i + 1) % log_interval:\n",
    "            _, top1 = acc_top1.get()\n",
    "            _, top5 = acc_top5.get()\n",
    "            err_top1, err_top5 = (1-top1, 1-top5)\n",
    "            print('Epoch[%d] Batch [%d]     Speed: %f samples/sec   top1-err=%f     top5-err=%f'%(\n",
    "                      epoch, i, batch_size/(time.time()-btic), err_top1, err_top5))\n",
    "            btic = time.time()\n",
    "\n",
    "    _, top1 = acc_top1.get()\n",
    "    _, top5 = acc_top5.get()\n",
    "    err_top1, err_top5 = (1-top1, 1-top5)\n",
    "\n",
    "    err_top1_val, err_top5_val = test(ctx, val_data)\n",
    "    train_history.update([err_top1, err_top5, err_top1_val, err_top5_val])\n",
    "\n",
    "    print('[Epoch %d] training: err-top1=%f err-top5=%f'%(epoch, err_top1, err_top5))\n",
    "    print('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n",
    "    print('[Epoch %d] validation: err-top1=%f err-top5=%f'%(epoch, err_top1_val, err_top5_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save net parameters\n",
    "current_model = 'edgeLite'\n",
    "# net1 = gluon.nn.HybridSequential()\n",
    "# net1.hybridize()\n",
    "# with net1.name_scope():\n",
    "#     net1.add(net)\n",
    "\n",
    "net.export(current_model, 10)\n",
    "saved_model = current_model + '.params'\n",
    "print (saved_model)\n",
    "net.save_parameters(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloaded input symbol and params files\n",
    "sym = './edgeLite-symbol.json'\n",
    "params = './edgeLite-0010.params'\n",
    "\n",
    "# Standard Imagenet input - 3 channels, 224*224\n",
    "input_shape = (1,3,224,224)\n",
    "\n",
    "# Path of the output file\n",
    "onnx_file = './edgeLite.onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet.contrib.onnx as onnx_mxnet\n",
    "converted_model_path = onnx_mxnet.export_model(sym, params, [input_shape], np.float32, onnx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(image, label):\n",
    "    resized = mx.image.resize_short(image, EDGE)\n",
    "    cropped, crop_info = mx.image.center_crop(resized, SIZE)\n",
    "    transposed = nd.transpose(cropped, (2,0,1))\n",
    "    return transposed, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data path of hazard dataset\n",
    "training_path = \"./data/101_ObjectCategories\"\n",
    "print(training_path)\n",
    "#testing data path of hazard dataset\n",
    "testing_path = \"./data/101_ObjectCategories_test\"\n",
    "print(testing_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.data.vision.datasets import ImageFolderDataset\n",
    "dataset_train = ImageFolderDataset(root=training_path)\n",
    "dataset_test = ImageFolderDataset(root=testing_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE = 224\n",
    "SIZE = (EDGE, EDGE)\n",
    "BATCH_SIZE = b_size\n",
    "NUM_WORKERS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.data import DataLoader\n",
    "\n",
    "dataloader_train_ft = DataLoader(dataset_train.transform(transform, lazy=False), batch_size=BATCH_SIZE, last_batch='rollover',\n",
    "                              shuffle=True, num_workers=NUM_WORKERS)\n",
    "dataloader_test_ft = DataLoader(dataset_test.transform(transform, lazy=False), batch_size=BATCH_SIZE, last_batch='rollover',\n",
    "                             shuffle=False, num_workers=NUM_WORKERS)\n",
    "print(\"Train dataset: {} images, Test dataset: {} images\".format(len(dataset_train), len(dataset_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = dataset_train.synsets\n",
    "NUM_CLASSES = len(categories)\n",
    "BATCH_SIZE = b_size\n",
    "#print(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 100\n",
    "plt.imshow((transform(dataset_train[N][0], 0)[0].asnumpy().transpose((1,2,0))))\n",
    "plt.axis('off')\n",
    "print(categories[dataset_train[N][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym, arg_params, aux_params = onnx_mxnet.import_model('edgeLite.onnx')\n",
    "def get_layer_output(symbol, arg_params, aux_params, layer_name):\n",
    "    all_layers = symbol.get_internals()\n",
    "    net = all_layers[layer_name+'_output']\n",
    "    net = mx.symbol.Flatten(data=net)\n",
    "    new_args = dict({k:arg_params[k] for k in arg_params if k in net.list_arguments()})\n",
    "    new_aux = dict({k:aux_params[k] for k in aux_params if k in net.list_arguments()})\n",
    "    return (net, new_args, new_aux)\n",
    "\n",
    "sym.get_internals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sym, new_arg_params, new_aux_params = get_layer_output(sym, arg_params, aux_params, 'flatten0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu() if mx.context.num_gpus() > 0 else mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    pre_trained = gluon.nn.SymbolBlock(outputs=new_sym, inputs=mx.sym.var('data'))\n",
    "\n",
    "net_params = pre_trained.collect_params()\n",
    "\n",
    "for param in new_arg_params:\n",
    "    if param in net_params:\n",
    "        net_params[param]._load_init(new_arg_params[param], ctx=ctx)\n",
    "for param in new_aux_params:\n",
    "    if param in net_params:\n",
    "        net_params[param]._load_init(new_aux_params[param], ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add new layer for fine-tuning\n",
    "NUM_CLASSES_NEW = 2\n",
    "dense_layer = gluon.nn.Dense(NUM_CLASSES_NEW)\n",
    "dense_layer.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluon.nn.HybridSequential()\n",
    "net.hybridize()\n",
    "with net.name_scope():\n",
    "    net.add(pre_trained)\n",
    "    net.add(dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "WDECAY = 0.0005\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu() \n",
    "net.collect_params().initialize(ctx=ctx)\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', \n",
    "                        {'learning_rate': LEARNING_RATE,\n",
    "                         'wd':WDECAY,\n",
    "                         'momentum':MOMENTUM})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gluon(data_iterator, net):\n",
    "    num_instance = 0\n",
    "    sum_metric = nd.zeros(1,ctx=ctx, dtype=np.int32)\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.astype(np.float32).as_in_context(ctx)\n",
    "        label = label.astype(np.int32).as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        prediction = nd.argmax(output, axis=1).astype(np.int32)\n",
    "        num_instance += len(prediction)\n",
    "        sum_metric += (prediction==label).sum()\n",
    "    accuracy = (sum_metric.astype(np.float32)/num_instance)\n",
    "    return accuracy.asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Untrained network Test Accuracy: {0:.4f}\".format(evaluate_accuracy_gluon(dataloader_test_ft, net)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon, nd, autograd\n",
    "\n",
    "val_accuracy = 0\n",
    "for epoch in range(15):\n",
    "    for i, (data, label) in enumerate(dataloader_train_ft):\n",
    "        data = data.astype(np.float32).as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "\n",
    "        if i%20==0 and i >0:\n",
    "            print('Batch [{0}] loss: {1:.4f}'.format(i, loss.mean().asscalar()))\n",
    "\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "    nd.waitall() # wait at the end of the epoch    \n",
    "    new_val_accuracy = evaluate_accuracy_gluon(dataloader_test_ft, net)    \n",
    "    print(\"Epoch [{0}] Test Accuracy {1:.4f} \".format(epoch, new_val_accuracy))\n",
    "\n",
    "    # We perform early-stopping regularization, to prevent the model from overfitting\n",
    "#     if val_accuracy > new_val_accuracy:\n",
    "#         print('Validation accuracy is decreasing, stopping training')\n",
    "#         break\n",
    "    val_accuracy = new_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save net parameters\n",
    "current_model = 'edgeLite_ft'\n",
    "# net1 = gluon.nn.HybridSequential()\n",
    "# net1.hybridize()\n",
    "# with net1.name_scope():\n",
    "#     net1.add(net)\n",
    "\n",
    "net.export(current_model, 10)\n",
    "saved_model = current_model + '.params'\n",
    "print (saved_model)\n",
    "net.save_parameters(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(img):\n",
    "    return nd.array(np.expand_dims(np.transpose(img, (2,0,1)),axis=0).astype(np.float32), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "#test images path\n",
    "path = './data/sdir/'\n",
    "script_path = '/usr/lib/python3/dist-packages/edgetpu/demo/'\n",
    "#folders = []\n",
    "\n",
    "files = os.listdir(path)\n",
    "#print (files)\n",
    "\n",
    "images = {}\n",
    "false_assum = {}\n",
    "\n",
    "for file in files:\n",
    "   if 'hazard' in file:\n",
    "      images.update({file : 'hazard'})\n",
    "   elif 'clean' in file:\n",
    "      images.update({file : 'clean'})\n",
    "\n",
    "#print (images)\n",
    "\n",
    "#total_time = time.time()\n",
    "for key in images:\n",
    "   print ('\\n\\nImage name : ',key, 'Tag: ' , images[key], 'floor and the classification result is : ' )\n",
    "   start_time = time.time()\n",
    "   img = cv2.imread(path+key)\n",
    "   img = cv2.resize(img,(224,224))\n",
    "   #img = np.reshape(img,[1,224,224,3])\n",
    "   #image = tf.cast(img, tf.float32)\n",
    "   image = transform(img)\n",
    "   classes = net(image)\n",
    "   print(classes)\n",
    "   if classes[0][0] > classes[0][1]:\n",
    "        res = 'clean'\n",
    "        print('clean')\n",
    "   else:\n",
    "        res = 'hazard'\n",
    "        print('hazard')\n",
    "   \n",
    "#    print(\"---%s seconds ---\" % (time.time() - start_time))\n",
    "   print(\"Image Name\", key ,\"classify as : \", res )\n",
    "   if images[key] in res:\n",
    "      images[key] = 1\n",
    "   else:\n",
    "      images[key] = 0\n",
    "      false_assum.update({key: 0})\n",
    "#print(\"---%s seconds ---\" % (time.time() - total_time))\n",
    "\n",
    "print (\"List of wrong assumtion : \", false_assum)\n",
    "print ('########## Final result #########')\n",
    "print ('Total right assumtion : ', sum(images.values()), '\\nTotal worng assumtion : ', len(images)-sum(images.values()), '\\nModel accuracy = ', round((sum(images.values())*100)/len(images), 3), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
